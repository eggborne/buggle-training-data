{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "\n",
    "def load_puzzle_data(file_path):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        puzzle_data = json.load(f)\n",
    "    return puzzle_data\n",
    "\n",
    "def extract_features(puzzles):\n",
    "    features = []\n",
    "    outcomes = []\n",
    "\n",
    "    for puzzle in puzzles:\n",
    "        matrix = puzzle['mtx']\n",
    "        total_words = puzzle['tw']\n",
    "        \n",
    "        # Extract features from the matrix\n",
    "        feature_dict = {\n",
    "            'num_rows': len(matrix),\n",
    "            'num_cols': len(matrix[0]),\n",
    "            'unique_letters': len(set(''.join(matrix))),\n",
    "            'vowels': sum(1 for row in matrix for char in row if char in 'aeiou'),\n",
    "            'consonants': sum(1 for row in matrix for char in row if char not in 'aeiou'),\n",
    "        }\n",
    "        \n",
    "        features.append(feature_dict)\n",
    "        outcomes.append(total_words)\n",
    "\n",
    "    return pd.DataFrame(features), np.array(outcomes)\n",
    "\n",
    "def preprocess_data(features, outcomes):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_model():\n",
    "    return RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mae, mse\n",
    "\n",
    "def plot_feature_importance(model, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    puzzle_data = load_puzzle_data('buggle-training-data/training_data.json')\n",
    "    print(f\"Got {len(puzzle_data)} puzzles.\")\n",
    "\n",
    "    # Extract features\n",
    "    features, outcomes = extract_features(puzzle_data)\n",
    "    print(f\"Extracted features for {len(features)} puzzles.\")\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(features, outcomes)\n",
    "    print(f\"Preprocessed {len(X_train)} training examples and {len(X_test)} test examples.\")\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model trained.\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    mae, mse = evaluate_model(model, X_test, y_test)\n",
    "    print(f\"Test MAE: {mae:.2f}\")\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "\n",
    "    # Plot feature importance\n",
    "    plot_feature_importance(model, features.columns)\n",
    "\n",
    "    # Save the model\n",
    "    dump(model, 'random_forest_puzzle_predictor.joblib')\n",
    "    print(\"Model saved as 'random_forest_puzzle_predictor.joblib'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
